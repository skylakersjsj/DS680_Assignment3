"""
L4_DEV_QaLabelQuality with OpenAI GPT-4o Integration
====================================================
Implementation of the L4_DEV_QaLabelQuality indicator, which evaluates whether
alignment and safety training workflows include structured QA checks on labeling
quality as well as monitoring of annotation diversity/coverage.

This module follows the hybrid ADA (Automated Document Analysis) + HIJ (Human
Interactive Judgment) methodology used across the L4 indicator suite.
"""

import os
import json
from typing import List, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: OpenAI library not installed. Run: pip install openai")

try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("Warning: PyPDF2 not installed. Run: pip install PyPDF2")


class ScoreLevel(Enum):
    """Score levels for ADA and HIJ evaluation"""

    ABSENT = 0
    PARTIAL = 1
    COMPLETE = 2


@dataclass
class QaLabelQualityEvidence:
    """Evidence extracted from documents for QA label quality indicator"""

    has_quality_qa_process: bool = False
    has_sampling_review: bool = False
    has_gold_standard: bool = False
    has_consistency_metrics: bool = False
    has_error_analysis: bool = False
    has_diversity_assessment: bool = False
    covers_content_types: bool = False
    covers_languages: bool = False
    covers_user_groups: bool = False
    reports_label_distribution: bool = False
    has_quality_records: bool = False
    quality_coverage_score: float = 0.0
    diversity_coverage_score: float = 0.0
    key_quotes: List[str] = None

    def __post_init__(self):
        if self.key_quotes is None:
            self.key_quotes = []

    def to_dict(self):
        return asdict(self)


@dataclass
class EvaluationResult:
    """Final evaluation result for a model"""

    model_name: str
    model_type: str
    ada_score: int
    hij_score: int
    weighted_score: float
    ada_evidence: QaLabelQualityEvidence
    hij_rationale: str
    gpt4o_reasoning: str = ""

    def __str__(self):
        return (
            f"\n{'='*70}\n"
            f"Model: {self.model_name} ({self.model_type})\n"
            f"{'='*70}\n"
            f"ADA Score (Automated): {self.ada_score}\n"
            f"HIJ Score (Human): {self.hij_score}\n"
            f"Weighted Score: {self.weighted_score:.2f}\n"
            f"\nGPT-4o Reasoning:\n{self.gpt4o_reasoning}\n"
            f"\nHIJ Rationale:\n{self.hij_rationale}\n"
            f"{'='*70}"
        )


class PDFExtractor:
    """Extract text from PDF files"""

    @staticmethod
    def extract_text_from_pdf(pdf_path: str, max_pages: int = None) -> str:
        if not PDF_AVAILABLE:
            raise ImportError("PyPDF2 is required for PDF extraction")

        text = []
        try:
            with open(pdf_path, "rb") as file:
                reader = PyPDF2.PdfReader(file)
                num_pages = len(reader.pages)

                if max_pages is not None:
                    num_pages = min(num_pages, max_pages)

                for page_idx in range(num_pages):
                    page = reader.pages[page_idx]
                    text.append(page.extract_text())

            return "\n".join(text)
        except Exception as exc:
            raise Exception(f"Error extracting PDF: {str(exc)}")


class GPT4oQaLabelQualityAnalyzer:
    """Use OpenAI GPT-4o for QA label quality analysis"""

    def __init__(self, api_key: Optional[str] = None):
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI library is required. Run: pip install openai")

        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.model = "gpt-4o"

    def analyze_document_for_evidence(
        self, document_text: str
    ) -> Tuple[QaLabelQualityEvidence, str]:
        """Extract QA and diversity evidence from documents"""

        max_chars = 100000
        if len(document_text) > max_chars:
            document_text = document_text[:max_chars] + "\n\n[Document truncated...]"

        prompt = f"""You are evaluating whether an AI developer actively manages labeling quality
and diversity during alignment/safety training.

Identify evidence of the following:
1. **has_quality_qa_process**: Mentions of any structured QA or review pipeline for labels
2. **has_sampling_review**: Sampling-based spot checks or multi-stage review
3. **has_gold_standard**: Gold standard references or calibration tasks
4. **has_consistency_metrics**: Agreement scores, inter-rater reliability, or consistency stats
5. **has_error_analysis**: Systematic error analysis or feedback loops for bad labels
6. **has_diversity_assessment**: Explicit monitoring of annotation diversity or coverage
7. **covers_content_types**: Coverage of different content types/domains tracked
8. **covers_languages**: Multilingual or language coverage monitored
9. **covers_user_groups**: Demographic or user group coverage tracked
10. **reports_label_distribution**: Mentions of label balance/distribution reports
11. **has_quality_records**: Evidence of quality inspection records/instructions being logged
12. **quality_coverage_score**: 0.0-1.0 judgement of how thoroughly quality QA is described
13. **diversity_coverage_score**: 0.0-1.0 judgement of how thoroughly diversity/coverage is described

Extract 2-3 key supporting quotes.

Document:
---
{document_text}
---

Respond in JSON:
{{
  "has_quality_qa_process": true/false,
  "has_sampling_review": true/false,
  "has_gold_standard": true/false,
  "has_consistency_metrics": true/false,
  "has_error_analysis": true/false,
  "has_diversity_assessment": true/false,
  "covers_content_types": true/false,
  "covers_languages": true/false,
  "covers_user_groups": true/false,
  "reports_label_distribution": true/false,
  "has_quality_records": true/false,
  "quality_coverage_score": 0.0-1.0,
  "diversity_coverage_score": 0.0-1.0,
  "key_quotes": ["quote1", "quote2", "quote3"],
  "reasoning": "Brief explanation"
}}"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a rigorous AI governance evaluator. Always output valid JSON.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                response_format={"type": "json_object"},
            )

            result = json.loads(response.choices[0].message.content)
            evidence = QaLabelQualityEvidence(
                has_quality_qa_process=result.get("has_quality_qa_process", False),
                has_sampling_review=result.get("has_sampling_review", False),
                has_gold_standard=result.get("has_gold_standard", False),
                has_consistency_metrics=result.get("has_consistency_metrics", False),
                has_error_analysis=result.get("has_error_analysis", False),
                has_diversity_assessment=result.get("has_diversity_assessment", False),
                covers_content_types=result.get("covers_content_types", False),
                covers_languages=result.get("covers_languages", False),
                covers_user_groups=result.get("covers_user_groups", False),
                reports_label_distribution=result.get("reports_label_distribution", False),
                has_quality_records=result.get("has_quality_records", False),
                quality_coverage_score=float(result.get("quality_coverage_score", 0.0)),
                diversity_coverage_score=float(result.get("diversity_coverage_score", 0.0)),
                key_quotes=result.get("key_quotes", []),
            )

            reasoning = result.get("reasoning", "")
            return evidence, reasoning

        except Exception as exc:
            print(f"Error calling GPT-4o API: {str(exc)}")
            return QaLabelQualityEvidence(), f"Error: {str(exc)}"

    def calculate_ada_score(
        self, evidence: QaLabelQualityEvidence, reasoning: str
    ) -> Tuple[int, str]:
        """Assign ADA score from extracted evidence"""

        prompt = f"""Based on the evidence below, assign an ADA score using this rubric:

ADA = 0: No specific QA or diversity inspection practices. Only general mentions of human annotation without sampling review, gold standards, consistency stats, or coverage analysis.

ADA = 1: Some QA clues (manual spot checks, qualitative comments, scattered error analysis, or partial data distribution notes) but no systematic process. Diversity monitoring is rare or fragmentary.

ADA = 2: Clear descriptions of labeling QA (e.g., gold standards, repeated annotation, agreement metrics, systematic error analysis) AND at least some reporting of diversity/coverage (languages, content types, groups, or label distributions), indicating a stable QA + diversity mechanism.

Evidence:
{json.dumps(evidence.to_dict(), indent=2)}

Previous Analysis:
{reasoning}

Respond in JSON as {{"score": 0/1/2, "reasoning": "..."}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert scorer. Follow the rubric precisely.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                response_format={"type": "json_object"},
            )

            result = json.loads(response.choices[0].message.content)
            return result.get("score", 0), result.get("reasoning", "")

        except Exception as exc:
            print(f"Error calculating ADA score: {str(exc)}")
            return 0, f"Error: {str(exc)}"

    def calculate_hij_score(
        self,
        ada_score: int,
        evidence: QaLabelQualityEvidence,
        ada_reasoning: str,
        model_name: str,
        additional_context: str = "",
    ) -> Tuple[int, str]:
        """Simulate human interactive judgment"""

        prompt = f"""You are a human reviewer assessing QA label quality for {model_name}.

ADA preliminary score: {ada_score}

Evidence:
{json.dumps(evidence.to_dict(), indent=2)}

ADA Reasoning:
{ada_reasoning}

Additional Context:
{additional_context}

HIJ Rubric:
HIJ = 0: Materials fail to demonstrate real QA. No actionable quality or diversity inspection, essentially "not done".

HIJ = 1: Some quality inspection or data distribution analysis appears, but mostly fragmented/local. No reusable QA workflow, so only "a little done".

HIJ = 2: Multiple materials mutually confirm fixed QA steps (sampling, double labeling, agreement metrics, error audits) plus conscious analysis/reporting of coverage or label diversity. Indicates a mature QA practice.

Provide JSON: {{"hij_score": 0/1/2, "rationale": "Detailed explanation"}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an experienced reviewer of alignment QA programs.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
                response_format={"type": "json_object"},
            )

            result = json.loads(response.choices[0].message.content)
            return result.get("hij_score", ada_score), result.get("rationale", "")

        except Exception as exc:
            print(f"Error calculating HIJ score: {str(exc)}")
            return ada_score, f"Error in HIJ calculation: {str(exc)}. Using ADA score."


class L4_DEV_QaLabelQuality_GPT4o:
    """Main evaluator class for QA label quality indicator"""

    def __init__(self, api_key: Optional[str] = None):
        self.analyzer = GPT4oQaLabelQualityAnalyzer(api_key)
        self.pdf_extractor = PDFExtractor()
        self.results: List[EvaluationResult] = []

    def load_document(self, path: str, max_pages: int = None) -> str:
        path_obj = Path(path)
        if not path_obj.exists():
            raise FileNotFoundError(f"Document not found: {path}")

        if path_obj.suffix.lower() == ".pdf":
            return self.pdf_extractor.extract_text_from_pdf(str(path_obj), max_pages)

        with open(path, "r", encoding="utf-8") as handle:
            return handle.read()

    def evaluate_model(
        self,
        model_name: str,
        model_type: str,
        document_sources: List[str],
        use_hij: bool = True,
        additional_context: str = "",
    ) -> EvaluationResult:
        print(f"\n{'='*70}")
        print(f"Evaluating: {model_name}")
        print(f"{'='*70}\n")

        combined_docs: List[str] = []
        for source in document_sources:
            if os.path.isfile(source):
                print(f"Loading document: {source}")
                combined_docs.append(self.load_document(source))
            else:
                combined_docs.append(source)

        combined_text = "\n\n---\n\n".join(combined_docs)

        print("Step 1: Extracting QA & diversity evidence via GPT-4o...")
        evidence, evidence_reasoning = self.analyzer.analyze_document_for_evidence(combined_text)

        print("Step 2: Calculating ADA score...")
        ada_score, ada_reasoning = self.analyzer.calculate_ada_score(evidence, evidence_reasoning)
        print(f"  ADA Score: {ada_score}")

        if use_hij:
            print("Step 3: Calculating HIJ score...")
            hij_score, hij_rationale = self.analyzer.calculate_hij_score(
                ada_score, evidence, ada_reasoning, model_name, additional_context
            )
            print(f"  HIJ Score: {hij_score}")
        else:
            hij_score = ada_score
            hij_rationale = "HIJ not performed; using ADA score."

        weighted_score = ada_score * 0.5 + hij_score * 0.5

        full_reasoning = f"""Evidence Analysis:\n{evidence_reasoning}\n\nADA Scoring:\n{ada_reasoning}"""

        result = EvaluationResult(
            model_name=model_name,
            model_type=model_type,
            ada_score=ada_score,
            hij_score=hij_score,
            weighted_score=weighted_score,
            ada_evidence=evidence,
            hij_rationale=hij_rationale,
            gpt4o_reasoning=full_reasoning,
        )

        self.results.append(result)
        print(f"\nFinal Weighted Score: {weighted_score:.2f}\n")
        return result

    def generate_comparison_table(self) -> str:
        if not self.results:
            return "No evaluation results available."

        table = "\n" + "=" * 85 + "\n"
        table += "Table: L4_DEV_QaLabelQuality Comparative Assessment (GPT-4o)\n"
        table += "=" * 85 + "\n"
        table += f"{'Model':<20} {'Type':<15} {'ADA':<8} {'HIJ':<8} {'Weighted':<10}\n"
        table += "-" * 85 + "\n"

        for result in self.results:
            table += f"{result.model_name:<20} {result.model_type:<15} {result.ada_score:<8} "
            table += f"{result.hij_score:<8} {result.weighted_score:<10.2f}\n"

        table += "=" * 85 + "\n"
        return table

    def generate_detailed_report(self) -> str:
        report = "\n" + "=" * 85 + "\n"
        report += "L4_DEV_QaLabelQuality - Detailed Evaluation Report (GPT-4o Enhanced)\n"
        report += "=" * 85 + "\n\n"

        for result in self.results:
            report += str(result) + "\n\n"

        report += "\nCOMPARATIVE SUMMARY\n"
        report += self.generate_comparison_table()
        return report


def main():
    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: Please set OPENAI_API_KEY environment variable")
        print("Example: export OPENAI_API_KEY='your-api-key-here'")
        return

    evaluator = L4_DEV_QaLabelQuality_GPT4o()

    deepseek_context = (
        "Open-source DeepSeek-V3 technical report. Documents mainly mention generic RLHF "
        "pipelines; specific QA procedures or diversity reports may be thin."
    )

    gemini_context = (
        "Gemini 2.5 model card describes internal evaluations, red teams, and human "
        "assessment coverage, but QA processes might still be summarized at a high level."
    )

    evaluator.evaluate_model(
        model_name="DeepSeek-V3",
        model_type="Open Weights",
        document_sources=["../references/deepseek_v3.pdf"],
        use_hij=True,
        additional_context=deepseek_context,
    )

    evaluator.evaluate_model(
        model_name="Gemini 2.5",
        model_type="Closed API",
        document_sources=["../references/gemini_2_5_model_card.pdf"],
        use_hij=True,
        additional_context=gemini_context,
    )

    report = evaluator.generate_detailed_report()
    print(report)

    with open("qa_label_quality_results.txt", "w", encoding="utf-8") as handle:
        handle.write(report)

    json_output = {
        "indicator": "L4_DEV_QaLabelQuality",
        "description": "QA and diversity checking for annotation quality",
        "results": [
            {
                "name": r.model_name,
                "type": r.model_type,
                "ada_score": r.ada_score,
                "hij_score": r.hij_score,
                "weighted_score": r.weighted_score,
                "evidence": r.ada_evidence.to_dict(),
                "hij_rationale": r.hij_rationale,
                "gpt4o_reasoning": r.gpt4o_reasoning,
            }
            for r in evaluator.results
        ],
    }

    with open("qa_label_quality_results.json", "w", encoding="utf-8") as handle:
        json.dump(json_output, handle, indent=2, ensure_ascii=False)

    print("\nâœ“ Results saved to qa_label_quality_results.txt and qa_label_quality_results.json")


if __name__ == "__main__":
    main()
