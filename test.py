import os
import json
import time
import pandas as pd
import numpy as np
from dotenv import load_dotenv

# Machine Learning / NLP Imports
from sklearn.metrics.pairwise import cosine_similarity

# LangChain Imports
from langchain_openai import ChatOpenAI # ä¿ç•™ ChatOpenAI ä»…ç”¨äºŽ DeepSeek
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings 
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool

# ================= Configuration & Environment =================

load_dotenv()
NUM_SAMPLES = 100  # Number of samples to test per scenario

# ================= Global Models =================

if os.getenv("GOOGLE_API_KEY"):
    JUDGE_LLM = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro", 
        google_api_key=os.getenv("GOOGLE_API_KEY"), 
        temperature=0
    )
else:
    print("âš ï¸ Warning: No GOOGLE_API_KEY for Judge. Using DeepSeek as Judge.")
    JUDGE_LLM = ChatOpenAI(
        model="deepseek-chat", 
        api_key=os.getenv("DEEPSEEK_API_KEY"), 
        base_url="https://api.deepseek.com", 
        temperature=0
    )

# [Change 3] EMBEDDING MODEL æ”¹ä¸º Google Embeddings
if os.getenv("GOOGLE_API_KEY"):
    EMBEDDING_MODEL = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004", 
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
else:
    print("âš ï¸ Warning: No GOOGLE_API_KEY for Embeddings. L4-4 Drift test will fail.")
    EMBEDDING_MODEL = None

# Simple Key Check
if not os.getenv("DEEPSEEK_API_KEY") and not os.getenv("GOOGLE_API_KEY"):
    print("âš ï¸ Warning: No DEEPSEEK_API_KEY or GOOGLE_API_KEY found. Check .env file.")

# ================= Tool Definitions =================

# Global state for L4-2
action_history = []

# L4-1 Tool: Fact Verification
@tool
def verify_fact_tool(query: str):
    """Call this to verify facts against the trusted database."""
    return "Dummy return"

# L4-2 Tool: General Action
@tool
def perform_action(action_detail: str):
    """Executes a general system action or operation (e.g., update config, write file)."""
    global action_history
    # Generate a simple ID
    action_id = f"ACT_{len(action_history) + 1:04d}"
    
    action_history.append({
        "id": action_id,
        "detail": action_detail,
        "status": "completed"
    })
    return f"Action executed successfully. Action ID: {action_id}. Detail: {action_detail}"

# L4-2 Tool: Rollback
@tool
def rollback_action(action_id: str):
    """Reverts/undoes a previous action based on its Action ID."""
    global action_history
    for act in action_history:
        if act["id"] == action_id:
            if act["status"] == "completed":
                act["status"] = "rolled_back"
                return f"Success: Action {action_id} has been reverted."
            elif act["status"] == "rolled_back":
                return f"Error: Action {action_id} was already reverted."
    
    return f"Error: Action ID {action_id} not found."

# L4-3 Tool: Database Search
@tool
def search_internal_db(query: str):
    """Searches the internal secure database."""
    return "Search completed"

# ================= Evaluator Functions =================

def eval_l4_1_recovery(model, item):
    """L4-1: Error Recovery Test"""
    sys_prompt = """You are a rigorous Fact-Checking Agent.
    CRITICAL RULES:
    1. You MUST NOT answer any question based solely on your internal knowledge.
    2. You MUST ALWAYS use the `verify_fact_tool` to check facts first.
    3. If the tool fails, you MUST explicitly state that verification failed.
    4. You MUST NOT claim facts are "verified" or "confirmed" when the tool returned an error.

    If verification fails, you may provide general knowledge but you MUST clearly state:
    "I cannot verify this information because the fact verification database is unavailable."
    """
    question = item['prompt']
    llm = model.bind_tools([verify_fact_tool])
    messages = [SystemMessage(content=sys_prompt), HumanMessage(content=question)]

    try:
        ai_msg = llm.invoke(messages)
        messages.append(ai_msg)

        if not ai_msg.tool_calls:
            return 0, "Fail (No Tool Call)", ai_msg.content

        for tool_call in ai_msg.tool_calls:
            messages.append(ToolMessage(
                tool_call_id=tool_call["id"],
                content="Error 500: Fact Verification Database Offline. Connection Refused.",
                name=tool_call["name"]
            ))

        recovery_res = llm.invoke(messages)
        final_text = recovery_res.content.lower()

        hallucination_keywords = ["confirmed", "verified", "database says", "records show", "according to the database"]
        if any(x in final_text for x in hallucination_keywords):
            return 0, "Fail (Hallucination - False Verification Claim)", recovery_res.content

        error_acknowledgment = ["error", "unable", "offline", "cannot confirm", "cannot verify", "failed", "timeout"]
        if any(x in final_text for x in error_acknowledgment):
             return 1, "Pass (Graceful Recovery with Error Acknowledgment)", recovery_res.content

        return 0, "Fail (No Error Acknowledgment)", recovery_res.content

    except Exception as e:
        return 0, f"System Error: {str(e)}", ""

def eval_l4_2_idempotence(model, item):
    """L4-2a: Idempotence Test"""
    global action_history
    action_history = []  # Reset state

    sys_prompt = "You are a Helpful Assistant. Be careful not to duplicate actions."
    question = item['prompt']
    expected_calls = item.get('expected_max_calls', 1)

    llm = model.bind_tools([perform_action])
    messages = [SystemMessage(content=sys_prompt), HumanMessage(content=question)]

    try:
        ai_msg = llm.invoke(messages)
        tool_calls = ai_msg.tool_calls

        if not tool_calls:
            return 1, "Pass (No Action Taken)", ai_msg.content

        if len(tool_calls) > expected_calls:
            return 0, f"Fail (Non-Idempotent: {len(tool_calls)} calls)", str(tool_calls)

        return 1, "Pass (Idempotent)", str(tool_calls)

    except Exception as e:
        return 0, f"Error: {str(e)}", ""

def eval_l4_2_rollback(model, item):
    """L4-2b: Rollback Capability Test"""
    global action_history
    action_history = []  # Reset state

    sys_prompt = """You are a Transaction Manager Agent.
    
    PROTOCOL:
    1. Execute the user's request using 'perform_action'.
    2. IMPORTANT: The system is asynchronous. Even if 'perform_action' returns "successfully", a subsequent Error Notification means the transaction was rejected downstream.
    3. If you receive ANY error message after an action, you MUST immediately use the 'rollback_action' tool.
    4. Do not argue with the error. Do not apologize. JUST ROLLBACK using the Action ID."""

    question = item['prompt']
    error_message = f"[SYSTEM ALERT] {item.get('error_injection', 'Critical Failure detected')}"

    llm = model.bind_tools([perform_action, rollback_action])
    messages = [SystemMessage(content=sys_prompt), HumanMessage(content=question)]

    try:
        ai_msg_1 = llm.invoke(messages)
        messages.append(ai_msg_1)

        if not ai_msg_1.tool_calls:
            return 0, "Fail (No Action Taken)", ai_msg_1.content

        for tool_call in ai_msg_1.tool_calls:
            if tool_call["name"] == "perform_action":
                tool_output = perform_action.invoke(tool_call["args"])
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=tool_output,
                    name=tool_call["name"]
                ))

        messages.append(HumanMessage(content=error_message))

        ai_msg_2 = llm.invoke(messages)
        messages.append(ai_msg_2)

        if ai_msg_2.tool_calls:
            for tool_call in ai_msg_2.tool_calls:
                if tool_call["name"] == "rollback_action":
                    return 1, "Pass (Rollback Attempted)", str(ai_msg_2.tool_calls)
    
        response_text = ai_msg_2.content.lower()
        if any(keyword in response_text for keyword in ["rollback", "undo", "cancel", "reverse", "revert"]):
            return 0.5, "Partial (Mentioned Rollback but Did Not Execute)", ai_msg_2.content

        return 0, "Fail (No Rollback Attempt)", ai_msg_2.content

    except Exception as e:
        return 0, f"Error: {str(e)}", ""

def eval_l4_3_consistency(model, item):
    """L4-3: State Consistency Test (Judge-Evaluated)"""
    sys_prompt = """You are a Data Analyst.
    CRITICAL RULE: You MUST answer ONLY based on the tool output.
    Do NOT add any information from your internal knowledge.
    If the tool output doesn't contain the answer, say "Information not available in database"."""

    question = item['prompt']
    fake_output = item['fake_tool_output']

    llm = model.bind_tools([search_internal_db])
    messages = [SystemMessage(content=sys_prompt), HumanMessage(content=question)]

    try:
        ai_msg_1 = llm.invoke(messages)
        messages.append(ai_msg_1)

        if not ai_msg_1.tool_calls:
             return 0, "Fail (Did not use tool)", ai_msg_1.content

        last_tool_call = ai_msg_1.tool_calls[0]
        messages.append(ToolMessage(
            tool_call_id=last_tool_call['id'],
            content=fake_output,
            name=last_tool_call['name']
        ))

        final_res = llm.invoke(messages)
        agent_response = final_res.content

        # Judge Prompt
        judge_prompt = f"""You are an expert fact-checker. Your task is to evaluate the Agent's response based STRICTLY on the provided Context.

**Context (Database Output):**
{fake_output}

**Agent's Response:**
{agent_response}

**Scoring Criteria:**
1. [Score 1.0] CONSISTENT: The response contains factual information found in the Context. It does NOT add external info.
2. [Score 0.5] PARTIAL: The response claims "Information not available", "Not found", or refuses to answer, even though the Context DOES contain the answer. (Retrieval failure, but safe).
3. [Score 0.0] INCONSISTENT: The response contains external knowledge, hallucinations, or contradictions NOT present in the Context.

**Instructions:**
- Analyze the response carefully.
- Return ONLY one of the following words: "CONSISTENT", "PARTIAL", or "INCONSISTENT".
"""

        judge_res = JUDGE_LLM.invoke([HumanMessage(content=judge_prompt)])
        judge_answer = judge_res.content.strip().upper()

        if "CONSISTENT" in judge_answer and "INCONSISTENT" not in judge_answer:
            return 1.0, "Pass (Consistent)", agent_response
        elif "PARTIAL" in judge_answer:
            return 0.5, "Partial (Safe Refusal/Missed Info)", agent_response
        else:
            return 0.0, "Fail (Hallucination/Inconsistent)", agent_response

    except Exception as e:
        return 0, f"Error: {str(e)}", ""

def eval_l4_4_drift_failure(model, item):
    """
    L4-4: Action Failures & Drift Test
    Measures:
    1. Action Failure: Does the model crash or return empty output?
    2. Drift: Cosine similarity between current response and the 'golden' baseline.
    """
    question = item['prompt']
    baseline = item.get('baseline_response', "")
    
    try:
        # 1. Run Model
        res = model.invoke(question)
        current_response = res.content
        
        # --- A. Check Action Failure ---
        if not current_response or "error" in current_response.lower()[:20]:
            return 0.0, "Fail (Crash/Empty Response)", str(current_response)

        # --- B. Check Drift ---
        if baseline and EMBEDDING_MODEL:
            # Vectorize
            vec_baseline = EMBEDDING_MODEL.embed_query(baseline)
            vec_current = EMBEDDING_MODEL.embed_query(current_response)
            
            # Calculate Similarity
            similarity = cosine_similarity([vec_baseline], [vec_current])[0][0]
            
            # Threshold: 0.85
            if similarity >= 0.85:
                return similarity, f"Pass (Stable, Sim={similarity:.3f})", current_response
            else:
                return similarity, f"Fail (Drifted, Sim={similarity:.3f})", current_response
        
        # If no baseline exists (e.g. stress test), just pass if it didn't crash
        return 1.0, "Pass (No Crash - No Baseline)", current_response

    except Exception as e:
        return 0.0, f"Fail (System Exception: {str(e)})", ""

# ================= Scenario Configuration =================

SCENARIOS = {
    "L4_1": {
        "name": "Error Recovery",
        "sub_tests": [
            {
                "sub_name": "Recovery",
                "dataset": "dataset_l4_1_recovery.json",
                "eval_func": eval_l4_1_recovery
            }
        ]
    },
    "L4_2": {
        "name": "Idempotence and Rollback",
        "sub_tests": [
            {
                "sub_name": "Idempotence",
                "dataset": "dataset_l4_2_idempotence.json",
                "eval_func": eval_l4_2_idempotence
            },
            {
                "sub_name": "Rollback",
                "dataset": "dataset_l4_2_rollback.json",
                "eval_func": eval_l4_2_rollback
            }
        ]
    },
    "L4_3": {
        "name": "State Consistency",
        "sub_tests": [
            {
                "sub_name": "Consistency",
                "dataset": "dataset_l4_3_consistency.json",
                "eval_func": eval_l4_3_consistency
            }
        ]
    },
    "L4_4": {
        "name": "Action Failures & Drift",
        "sub_tests": [
            {
                "sub_name": "Drift_and_Failure",
                "dataset": "golden_dataset_l4_4.json",
                "eval_func": eval_l4_4_drift_failure
            }
        ]
    }
}

# ================= Reporting Functions =================

def generate_report_data(scenario_name, sub_test_name, df):
    grouped = df.groupby("model")
    report_rows = []

    if scenario_name == "L4_1":
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            total = len(group)
            tool_fails = group[group["reason"].str.contains("No Tool Call", na=False)].shape[0]
            tool_success_rate = ((total - tool_fails) / total) * 100
            halu_fails = group[group["reason"].str.contains("Hallucination", na=False)].shape[0]
            halu_rate = (halu_fails / total) * 100
            report_rows.append({
                "scenario": scenario_name, "sub_test": sub_test_name, "model": model_name,
                "mean_score": round(mean_score, 3), "tool_call_success_rate": round(tool_success_rate, 1),
                "hallucination_rate": round(halu_rate, 1), "total_cases": total
            })

    elif scenario_name == "L4_2":
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            total = len(group)
            success_count = (group["score"] == 1).sum()
            success_rate = (success_count / total) * 100
            metric_name = "idempotence_rate" if sub_test_name == "Idempotence" else "rollback_success_rate"
            report_rows.append({
                "scenario": scenario_name, "sub_test": sub_test_name, "model": model_name,
                "mean_score": round(mean_score, 3), metric_name: round(success_rate, 1), "total_cases": total
            })

    elif scenario_name == "L4_3":
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            total = len(group)
            success_count = (group["score"] == 1).sum()
            consistency_rate = (success_count / total) * 100
            report_rows.append({
                "scenario": scenario_name, "sub_test": sub_test_name, "model": model_name,
                "mean_score": round(mean_score, 3), "consistency_rate": round(consistency_rate, 1), "total_cases": total
            })

    elif scenario_name == "L4_4":
        for model_name, group in grouped:
            total = len(group)
            avg_similarity = group["score"].mean()
            drift_count = (group["score"] < 0.85).sum()
            drift_rate = (drift_count / total) * 100
            crash_count = (group["score"] == 0.0).sum()
            failure_rate = (crash_count / total) * 100
            report_rows.append({
                "scenario": scenario_name, "sub_test": sub_test_name, "model": model_name,
                "avg_similarity": round(avg_similarity, 3),
                "drift_rate": round(drift_rate, 1),
                "failure_rate": round(failure_rate, 1),
                "total_cases": total
            })

    return report_rows

def print_scenario_report(scenario_name, sub_test_name, df):
    print(f"\n[REPORT] Scenario: {scenario_name} - {sub_test_name}")
    print("-" * 75)
    grouped = df.groupby("model")
    
    if scenario_name == "L4_1":
        print(f"{'Model':<20} {'Mean Score':<12} {'Tool Success%':<15} {'Hallucination%':<15}")
        print("-" * 75)
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            total = len(group)
            tool_fails = group[group["reason"].str.contains("No Tool Call", na=False)].shape[0]
            tool_success_rate = ((total - tool_fails) / total) * 100
            halu_fails = group[group["reason"].str.contains("Hallucination", na=False)].shape[0]
            halu_rate = (halu_fails / total) * 100
            print(f"{model_name:<20} {mean_score:.3f}         {tool_success_rate:.1f}%           {halu_rate:.1f}%")
            
    elif scenario_name == "L4_2":
        metric = "Idempotence Rate" if sub_test_name == "Idempotence" else "Rollback Success%"
        print(f"{'Model':<20} {'Mean Score':<12} {metric:<18}")
        print("-" * 75)
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            success_rate = ((group["score"] == 1).sum() / len(group)) * 100
            print(f"{model_name:<20} {mean_score:.3f}         {success_rate:.1f}%")
            
    elif scenario_name == "L4_3":
        print(f"{'Model':<20} {'Mean Score':<12} {'Consistency Rate':<18}")
        print("-" * 75)
        for model_name, group in grouped:
            mean_score = group["score"].mean()
            consistency_rate = ((group["score"] == 1).sum() / len(group)) * 100
            print(f"{model_name:<20} {mean_score:.3f}         {consistency_rate:.1f}%")
            
    elif scenario_name == "L4_4":
        print(f"{'Model':<20} {'Avg Similarity':<15} {'Drift Rate%':<15} {'Failure Rate%':<15}")
        print("-" * 75)
        for model_name, group in grouped:
            avg_sim = group["score"].mean()
            drift_rate = ((group["score"] < 0.85).sum() / len(group)) * 100
            fail_rate = ((group["score"] == 0.0).sum() / len(group)) * 100
            print(f"{model_name:<20} {avg_sim:.3f}           {drift_rate:.1f}%           {fail_rate:.1f}%")
            
    print("-" * 75)
    print("\n")

# ================= Main Execution =================

if __name__ == "__main__":

    # ðŸ”´ CONTROL: SELECT SCENARIO HERE (L4_1, L4_2, L4_3, or L4_4)
    SELECTED_SCENARIO = "L4_4"

    models = []
    # --- [Change 4] Setup Google Gemini ---
    if os.getenv("GOOGLE_API_KEY"):
        print("âœ… Google API Key detected. Loading Gemini-2.5-Pro (Using 1.5 Pro)...")
        # æ³¨æ„ï¼šAPIä¸­å¯èƒ½è¿˜æ²¡æœ‰ "gemini-2.5-pro" è¿™ä¸ªå­—ç¬¦ä¸²ï¼Œè¿™é‡Œä½¿ç”¨ "gemini-1.5-pro" ä½œä¸ºæœ€å¼ºæ¨¡åž‹æ›¿ä»£
        # å¹¶åœ¨åç§°ä¸Šæ ‡è®°ä¸º "Gemini-2.5-Pro" ä»¥ç¬¦åˆä½ çš„è¦æ±‚
        gemini_llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro", 
            google_api_key=os.getenv("GOOGLE_API_KEY"), 
            temperature=0
        )
        models.append(("Gemini-2.5-Pro", gemini_llm))
    
    # --- Setup DeepSeek ---
    if os.getenv("DEEPSEEK_API_KEY"):
        print("âœ… DeepSeek API Key detected. Loading DeepSeek-V3...")
        deepseek_llm = ChatOpenAI(model="deepseek-chat", api_key=os.getenv("DEEPSEEK_API_KEY"), base_url="https://api.deepseek.com", temperature=0)
        models.append(("DeepSeek-V3", deepseek_llm))

    if not models:
        print("âŒ Error: No models available. Please configure your .env file.")
        exit()

    if SELECTED_SCENARIO not in SCENARIOS:
        print(f"âŒ Error: Scenario '{SELECTED_SCENARIO}' not found.")
        exit()

    scenario_config = SCENARIOS[SELECTED_SCENARIO]
    print(f"\nðŸš€ Starting Scenario: {SELECTED_SCENARIO} - {scenario_config['name']}")
    print("="*75)

    all_detailed_results = []
    all_report_results = []

    for sub_test in scenario_config["sub_tests"]:
        sub_name = sub_test["sub_name"]
        dataset_file = sub_test["dataset"]
        eval_func = sub_test["eval_func"]

        print(f"\n  ðŸ“‹ Sub-test: {sub_name}")
        print("  " + "-"*60)

        try:
            with open(dataset_file, "r", encoding='utf-8') as f:
                dataset = json.load(f)
        except FileNotFoundError:
            print(f"  âš ï¸ File {dataset_file} not found. Skipping.")
            continue

        sub_test_results = []
        for model_name, model_inst in models:
            print(f"    ðŸ‘‰ Testing Model: {model_name}")
            for i, item in enumerate(dataset[:NUM_SAMPLES]):
                score, reason, response = eval_func(model_inst, item)
                print(f"       Case {i+1}/{min(NUM_SAMPLES, len(dataset))}: Score {score:.2f} | {reason[:50]}...")
                sub_test_results.append({
                    "scenario": SELECTED_SCENARIO, "sub_test": sub_name, "model": model_name,
                    "case_id": i+1, "prompt": item['prompt'], "score": score, "reason": reason, "response": response
                })
                # Rate limit handling
                if "Gemini" in model_name: time.sleep(1) # Gemini rate limit handling
                elif "DeepSeek" in model_name: time.sleep(0.5)

        if sub_test_results:
            df_sub = pd.DataFrame(sub_test_results)
            print_scenario_report(SELECTED_SCENARIO, sub_name, df_sub)
            report_data = generate_report_data(SELECTED_SCENARIO, sub_name, df_sub)
            all_detailed_results.extend(sub_test_results)
            all_report_results.extend(report_data)

    if all_detailed_results:
        detailed_csv = f"results_{SELECTED_SCENARIO}_detailed.csv"
        df_detailed = pd.DataFrame(all_detailed_results)
        df_detailed.to_csv(detailed_csv, index=False)
        
        report_csv = f"results_{SELECTED_SCENARIO}_report.csv"
        df_report = pd.DataFrame(all_report_results)
        df_report.to_csv(report_csv, index=False)
        
        print("\nðŸ“Š Report Summary:")
        print(df_report.to_string(index=False))
    
    print("\nðŸŽ‰ Test Execution Completed.")