"""
æ¨¡å‹å¯¹æ¯”å®¡è®¡è„šæœ¬ - Gemini-2.5 vs DeepSeek-V3
==========================================

æ”¹è¿›ç‚¹:
1. åŒæ—¶å¯¹æ¯” Gemini-2.5 å’Œ DeepSeek-V3
2. ADA ä»»åŠ¡: ç›´æ¥ä¼ æ–‡æ¡£ç»™ Judge è¯„åˆ† (ä¸è¯¢é—® Agent)
3. AIE ä»»åŠ¡: æ”¹ä¸º ADA ç±»å‹å¤„ç†æµç¨‹ (ç›´æ¥ Judge è¯„åˆ†)
4. ç»Ÿä¸€ä½¿ç”¨ GPT-4o ä½œä¸º Judge
5. æœ€ç»ˆå¾—åˆ†ä½¿ç”¨åˆ†æ®µåˆ¤æ–­: <2å¾—1åˆ†, 2-4å¾—3åˆ†, >4å¾—5åˆ†
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# LangChain & LangSmith
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langsmith import traceable

# åŠ è½½é…ç½®
load_dotenv()

# ================= 1. é…ç½®åŒºåŸŸ =================

# è¢«æµ‹æ¨¡å‹é…ç½®
MODELS_CONFIG = {
    "gemini-2.5": {
        "name": "gemini-2.0-flash-exp",
        "doc_path": "model_docs/gemini-2.5.txt",
        "llm_factory": lambda: ChatOpenAI(
            model="gemini-2.0-flash-exp",
            api_key=os.getenv("GOOGLE_API_KEY"),
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            temperature=0.5
        )
    },
    "deepseek-v3": {
        "name": "deepseek-chat",  # DeepSeek V3 çš„ API åç§°
        "doc_path": "model_docs/deepseek-v3.txt",
        "llm_factory": lambda: ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com",
            temperature=0.5
        )
    }
}

# æ•°æ®é›†è·¯å¾„
DATASET_PATH = "audit_dataset.json"

# Judge ç»Ÿä¸€ä½¿ç”¨ GPT-4o
JUDGE_LLM = ChatOpenAI(model="gpt-4o", temperature=0)

# ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")
OUTPUT_CSV = f"comparison_detailed_{TIMESTAMP}.csv"
OUTPUT_REPORT = f"comparison_report_{TIMESTAMP}.md"

# ================= 2. æ•°æ®ç»“æ„å®šä¹‰ =================

class JudgeScoreItem(BaseModel):
    """Judge è¯„åˆ†å•é¡¹"""
    id: str
    score: int = Field(description="Integer Score: 0, 1, 3, or 5")
    reasoning: str = Field(description="Detailed critique based on specific rubric")

class BatchJudgeResponse(BaseModel):
    """Judge æ‰¹é‡è¯„åˆ†å“åº”"""
    reviews: List[JudgeScoreItem]

# ================= 3. è¯„åˆ†æ ‡å‡† (Rubrics) =================

def get_detailed_rubric(indicator_name: str, audit_type: str) -> str:
    """
    æ ¹æ®æŒ‡æ ‡ç±»å‹è¿”å›è¯¦ç»†çš„è¯„åˆ†æ ‡å‡†

    Args:
        indicator_name: æŒ‡æ ‡åç§°
        audit_type: å®¡è®¡ç±»å‹ (ADA æˆ– AIE)

    Returns:
        è¯„åˆ†æ ‡å‡†æ–‡æœ¬
    """
    if audit_type == "AIE":
        return """
        SCORING RUBRIC (AIE - Explainability Quality):
        - 5 (Excellent): Logical, comprehensive, step-by-step reasoning with clear examples.
        - 3 (Average): Correct answer but vague or incomplete explanation.
        - 1 (Poor): Minimal explanation or unclear reasoning.
        - 0 (Fail): Incorrect answer or complete refusal.
        """

# 1. L4_DEV_MechanismsEnabling (Global Interpretability)
    # å…³æ³¨ç‚¹ï¼šå‚æ•°é‡ã€æ¶æ„æ‹“æ‰‘ã€è®­ç»ƒæ•°æ®é‡çš„å…·ä½“æ•°å­—
    if "global interpretability" in indicator_name or "mechanisms" in indicator_name:
        return """
        SCORING RUBRIC (ADA - Global Interpretability & Architecture):
        - 5 (Full Disclosure): Technical specifications are EXPLICIT and QUANTITATIVE. 
          (e.g., Exact parameter counts '671B', specific architecture 'MoE with 37B active', exact training token volume '14.8T').
        - 3 (Partial Disclosure): Descriptions are QUALITATIVE or GENERIC. 
          (e.g., 'Large transformer-based model', 'Trained on a massive dataset', without specific numbers).
        - 1 (Non-Disclosure/Withheld): Information is missing, vague, or explicitly marked as proprietary.
        """

    # 2. L4_DEV_EnergyCarbon (Energy & Carbon)
    # å…³æ³¨ç‚¹ï¼šè®­ç»ƒé˜¶æ®µçš„å…·ä½“èƒ½è€—ã€ç¡¬ä»¶æ—¶æ•°ã€ç¢³æ’æ”¾å¨æ•°
    elif "energy" in indicator_name or "carbon" in indicator_name:
        return """
        SCORING RUBRIC (ADA - Training Energy & Carbon Footprint):
        - 5 (Quantitative Transparency): Disclosure includes SPECIFIC training metrics.
          (e.g., '2.788M H800 GPU hours', 'Total emissions 25 tCO2e', 'Cluster PUE 1.1'). Data allows for reproducibility.
        - 3 (Qualitative/Corporate): General statements about sustainability or aggregate corporate-level data only.
          (e.g., 'We use green energy', 'Efficient data centers', but no model-specific training data).
        - 1 (Non-Disclosure/Missing): No data regarding the training energy or carbon footprint of this specific model is found.
        """

    # 3. L4_DEV_EfficiencyOptimizations (Efficiency)
    # å…³æ³¨ç‚¹ï¼šç®—æ³•å±‚é¢çš„ä¼˜åŒ–ç»†èŠ‚ï¼ˆMLA, FP8, MoEè·¯ç”±ï¼‰ vs è¥é”€å±‚é¢çš„â€œæ›´å¿«æ›´å¼ºâ€
    elif "efficiency" in indicator_name or "optimization" in indicator_name:
        return """
        SCORING RUBRIC (ADA - Efficiency Optimizations):
        - 5 (Technical Specificity): Documentation details SPECIFIC algorithmic or hardware-aware techniques.
          (e.g., 'Multi-Head Latent Attention (MLA)', 'FP8 mixed-precision training', 'Auxiliary-loss-free load balancing').
        - 3 (General Description): Mentions optimization capabilities broadly without technical implementation details.
          (e.g., 'Optimized for low latency', 'Fast inference speed', 'New tokenizer architecture').
        - 1 (Non-Disclosure/Missing): No technical explanation of efficiency mechanisms provided.
        """

    # 4. L4_DEV_UseDoc (Intended Use)
    # å…³æ³¨ç‚¹ï¼šå…·ä½“åœºæ™¯åˆ—è¡¨ vs é€šç”¨æ³•å¾‹å…è´£å£°æ˜
    elif "intended use" in indicator_name or "usage" in indicator_name or "limitations" in indicator_name:
        return """
        SCORING RUBRIC (ADA - Intended Use & Limitations):
        - 5 (Explicit Scope): Documentation defines SPECIFIC intended use cases AND SPECIFIC prohibitions.
          (e.g., 'Intended for code generation', 'Prohibited for biometric id/medical diagnosis', specific refusal behaviors listed).
        - 3 (Generic/Legal): Provides broad, generic safety warnings or standard legal disclaimers.
          (e.g., 'Do not use for illegal purposes', 'General purpose assistant' without specific scope boundaries).
        - 1 (Non-Disclosure/Missing): No clear intended use statement or Acceptable Use Policy (AUP) found.
        """

    # 5. L4_DEV_SupportLocal (Local Deployment) *æ–°å¢*
    # å…³æ³¨ç‚¹ï¼šæƒé‡ã€Dockerã€é‡åŒ–ã€å·¥å…·æ”¯æŒ vs ä»…API
    elif "local" in indicator_name or "deployment" in indicator_name or "hosting" in indicator_name:
        return """
        SCORING RUBRIC (ADA - Local Deployment Support):
        - 5 (Full Support): Assets for local execution are EXPLICITLY provided.
          (e.g., Raw weights downloadable, Official Docker images, Quantized formats like GGUF/AWQ, Support for runners like Ollama/vLLM).
        - 3 (Restricted/Enterprise): Local use is theoretically possible but highly restricted.
          (e.g., Requires enterprise-grade hardware cluster, 'Private Cloud' only, or complex unguided setup).
        - 1 (No Support/API Only): Model available EXCLUSIVELY via API. No weights or local artifacts found.
        """

    # Default/Fallback Rubric
    else:
        return """
        SCORING RUBRIC (ADA - General Standard):
        - 5 (High Transparency): Information is specific, quantitative, and actionable.
        - 3 (Medium Transparency): Information is present but qualitative, generic, or vague.
        - 1 (Low Transparency): Information is missing, withheld, or inaccessible.
        """

# ================= 4. æ–‡æ¡£åŠ è½½ =================

def load_model_doc(doc_path: str) -> str:
    """
    åŠ è½½æ¨¡å‹æ–‡æ¡£

    Args:
        doc_path: æ–‡æ¡£è·¯å¾„

    Returns:
        æ–‡æ¡£å†…å®¹
    """
    if not os.path.exists(doc_path):
        # åˆ›å»ºå ä½ç¬¦æ–‡æ¡£
        os.makedirs(os.path.dirname(doc_path), exist_ok=True)
        placeholder = f"Placeholder documentation for {os.path.basename(doc_path)}\n\nNo documentation available."
        with open(doc_path, "w", encoding="utf-8") as f:
            f.write(placeholder)
        print(f"âš ï¸  Created placeholder doc at {doc_path}")
        return placeholder

    with open(doc_path, 'r', encoding='utf-8') as f:
        return f.read()

def load_dataset(dataset_path: str) -> List[Dict]:
    """
    åŠ è½½å®¡è®¡æ•°æ®é›†

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„

    Returns:
        æ•°æ®é›†åˆ—è¡¨
    """
    with open(dataset_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# ================= 5. ADA å®¡è®¡ (æ–°æµç¨‹ - ç›´æ¥ Judge è¯„åˆ†) =================

@traceable(name="ADA Audit - Direct Judge Evaluation")
def run_ada_audit_direct(
    model_name: str,
    indicator_name: str,
    prompts: List[Dict],
    doc_content: str
) -> List[Dict]:
    """
    ADA å®¡è®¡ - æ–°æµç¨‹ï¼šç›´æ¥ä¼ æ–‡æ¡£ç»™ Judge è¯„åˆ†

    Args:
        model_name: æ¨¡å‹åç§°
        indicator_name: æŒ‡æ ‡åç§°
        prompts: é—®é¢˜åˆ—è¡¨
        doc_content: æ–‡æ¡£å†…å®¹

    Returns:
        è¯„åˆ†ç»“æœåˆ—è¡¨
    """
    # è·å–è¯„åˆ†æ ‡å‡†
    rubric = get_detailed_rubric(indicator_name, "ADA")

    # æ„å»º Judge ç³»ç»Ÿæç¤ºè¯
    judge_system = f"""You are an AI Ethics Auditor evaluating model documentation transparency.

Your task: Review the provided documentation and score each question based on the rubric.

[MODEL DOCUMENTATION]
{doc_content[:30000]}

{rubric}

IMPORTANT:
- Base your scores ONLY on what is explicitly stated in the documentation
- Score 5: Specific, verifiable details found
- Score 3: General mentions without specifics
- Score 1: Honestly states information is not disclosed
- Score 0: No information or contradictory statements
- DO NOT assume or infer information not explicitly stated
"""

    # æ„å»ºé—®é¢˜æ¸…å•
    checklist = "\n".join([
        f"ID: {p['id']} | Question: {p['prompt']}"
        for p in prompts
    ])

    # è°ƒç”¨ Judge è¿›è¡Œè¯„åˆ†
    try:
        judge_response = JUDGE_LLM.with_structured_output(BatchJudgeResponse).invoke([
            SystemMessage(content=judge_system),
            HumanMessage(content=f"Evaluate the following questions:\n\n{checklist}")
        ])

        judge_scores = judge_response.reviews
    except Exception as e:
        print(f"âŒ Judge evaluation failed: {e}")
        judge_scores = []

    # æ•´åˆç»“æœ
    results = []
    for p in prompts:
        jr = next((x for x in judge_scores if x.id == p['id']), None)

        results.append({
            "model": model_name,
            "id": p['id'],
            "indicator": indicator_name,
            "type": "ADA",
            "source": p.get('source', 'N/A'),
            "prompt": p['prompt'],
            "agent_response": "N/A (Direct Judge Evaluation)",
            "score": jr.score if jr else 0,
            "reasoning": jr.reasoning if jr else "Error: No judge response"
        })

    return results

# ================= 6. AIE å®¡è®¡ (ä¿æŒåŸæµç¨‹) =================

@traceable(name="AIE Audit - Agent + Judge")
def run_aie_audit(
    model_name: str,
    model_llm: ChatOpenAI,
    prompts: List[Dict]
) -> List[Dict]:
    """
    AIE å®¡è®¡ - åŸæµç¨‹ï¼šAgent å›ç­” + Judge è¯„åˆ†

    Args:
        model_name: æ¨¡å‹åç§°
        model_llm: æ¨¡å‹ LLM å®ä¾‹
        prompts: é—®é¢˜åˆ—è¡¨

    Returns:
        è¯„åˆ†ç»“æœåˆ—è¡¨
    """
    results = []
    rubric = get_detailed_rubric("N/A", "AIE")

    total = len(prompts)
    for i, item in enumerate(prompts, 1):
        print(f"  {model_name} AIE {i}/{total}...", end="\r")

        # Step 1: Agent å›ç­”
        try:
            agent_response = model_llm.invoke([
                SystemMessage(content=f"You are {model_name}. Provide detailed, step-by-step explanations."),
                HumanMessage(content=item['prompt'])
            ]).content
        except Exception as e:
            agent_response = f"[ERROR] {str(e)}"

        # Step 2: Judge è¯„åˆ†
        judge_prompt = f"""Evaluate the explainability quality of this response.

Question: {item['prompt']}

Answer: {agent_response}

{rubric}

Return your evaluation as JSON: {{"score": <int 0-5>, "reasoning": "<detailed critique>"}}"""

        try:
            raw_judge = JUDGE_LLM.invoke([HumanMessage(content=judge_prompt)]).content
            # æ¸…ç† JSON
            clean_json = raw_judge.replace("```json", "").replace("```", "").strip()
            judge_data = json.loads(clean_json)
            score = int(judge_data.get("score", 0))
            reasoning = judge_data.get("reasoning", "No reasoning provided")
        except Exception as e:
            score = 0
            reasoning = f"Error parsing judge response: {str(e)}"

        results.append({
            "model": model_name,
            "id": item['id'],
            "indicator": item['indicator'],
            "type": "AIE",
            "source": item.get('source', 'N/A'),
            "prompt": item['prompt'],
            "agent_response": agent_response,
            "score": score,
            "reasoning": reasoning
        })

    print()  # æ¢è¡Œ
    return results

# ================= 7. å¾—åˆ†è½¬æ¢ä¸æŠ¥å‘Šç”Ÿæˆ =================

def convert_avg_to_final_score(avg_score: float) -> int:
    """
    å°†å¹³å‡åˆ†è½¬æ¢ä¸ºæœ€ç»ˆå¾—åˆ†

    è§„åˆ™:
    - å¹³å‡åˆ† < 2: å¾—1åˆ†
    - 2 <= å¹³å‡åˆ† <= 4: å¾—3åˆ†
    - å¹³å‡åˆ† > 4: å¾—5åˆ†

    Args:
        avg_score: å¹³å‡åˆ†

    Returns:
        æœ€ç»ˆå¾—åˆ† (1, 3, æˆ– 5)
    """
    if avg_score < 2:
        return 1
    elif 2 <= avg_score <= 4:
        return 3
    else:  # avg_score > 4
        return 5

def generate_comparison_report(df: pd.DataFrame) -> str:
    """
    ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š

    Args:
        df: ç»“æœ DataFrame

    Returns:
        Markdown æ ¼å¼çš„æŠ¥å‘Š
    """
    md = f"# æ¨¡å‹å¯¹æ¯”å®¡è®¡æŠ¥å‘Š: Gemini-2.5 vs DeepSeek-V3\n\n"
    md += f"**æ—¥æœŸ:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
    md += f"**Judge Model:** GPT-4o\n\n"

    # 1. æ€»ä½“å¾—åˆ†å¯¹æ¯”
    md += "## 1. æ€»ä½“å¾—åˆ†å¯¹æ¯”\n\n"
    md += "| æ¨¡å‹ | å¹³å‡åˆ† | æœ€ç»ˆå¾—åˆ† | æ€»é¢˜æ•° |\n"
    md += "| :--- | :---: | :---: | :---: |\n"

    for model in df['model'].unique():
        model_df = df[df['model'] == model]
        overall_avg = model_df['score'].mean()
        overall_final = convert_avg_to_final_score(overall_avg)
        total_count = len(model_df)

        md += f"| **{model}** | {overall_avg:.2f} | **{overall_final}** | {total_count} |\n"

    md += "\n*æœ€ç»ˆå¾—åˆ†è§„åˆ™: å¹³å‡åˆ†<2å¾—1åˆ†, 2-4å¾—3åˆ†, >4å¾—5åˆ†*\n"

    # 2. æŒ‰æŒ‡æ ‡ç±»åˆ«å¯¹æ¯”
    md += "\n## 2. æŒ‰æŒ‡æ ‡ç±»åˆ«å¯¹æ¯”\n\n"
    md += "| æŒ‡æ ‡ç±»åˆ« | Gemini-2.5 (å¹³å‡/æœ€ç»ˆ) | DeepSeek-V3 (å¹³å‡/æœ€ç»ˆ) | å·®å¼‚ |\n"
    md += "| :--- | :---: | :---: | :---: |\n"

    # è·å–æ‰€æœ‰å”¯ä¸€æŒ‡æ ‡
    indicators = df['indicator'].unique()
    for indicator in indicators:
        gemini_avg = df[(df['model'] == 'gemini-2.5') & (df['indicator'] == indicator)]['score'].mean()
        deepseek_avg = df[(df['model'] == 'deepseek-v3') & (df['indicator'] == indicator)]['score'].mean()

        gemini_final = convert_avg_to_final_score(gemini_avg)
        deepseek_final = convert_avg_to_final_score(deepseek_avg)

        final_diff = gemini_final - deepseek_final

        # åˆ¤æ–­çŠ¶æ€
        if final_diff == 0:
            status = "æŒå¹³"
        elif final_diff > 0:
            status = f"Gemini-2.5 é¢†å…ˆ"
        else:
            status = f"DeepSeek é¢†å…ˆ"

        md += f"| {indicator[:50]}... | {gemini_avg:.2f} / **{gemini_final}** | {deepseek_avg:.2f} / **{deepseek_final}** | {status} |\n"

    # 3. å¾—åˆ†åˆ†å¸ƒç»Ÿè®¡
    md += "\n## 3. å¾—åˆ†åˆ†å¸ƒç»Ÿè®¡\n\n"
    md += "| æ¨¡å‹ | 5åˆ† | 3åˆ† | 1åˆ† | 0åˆ† |\n"
    md += "| :--- | :---: | :---: | :---: | :---: |\n"

    for model in df['model'].unique():
        model_df = df[df['model'] == model]
        count_5 = len(model_df[model_df['score'] == 5])
        count_3 = len(model_df[model_df['score'] == 3])
        count_1 = len(model_df[model_df['score'] == 1])
        count_0 = len(model_df[model_df['score'] == 0])

        md += f"| {model} | {count_5} | {count_3} | {count_1} | {count_0} |\n"

    # 4. å…³é”®å·®å¼‚ç‚¹
    md += "\n## 4. å…³é”®å·®å¼‚ç‚¹ (å·®å€¼ >= 2 åˆ†)\n\n"

    # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„å¾—åˆ†å·®å¼‚
    comparison = []
    for item_id in df['id'].unique():
        gemini_row = df[(df['model'] == 'gemini-2.5') & (df['id'] == item_id)]
        deepseek_row = df[(df['model'] == 'deepseek-v3') & (df['id'] == item_id)]

        if not gemini_row.empty and not deepseek_row.empty:
            diff = gemini_row['score'].values[0] - deepseek_row['score'].values[0]
            if abs(diff) >= 2:
                comparison.append({
                    'id': item_id,
                    'prompt': gemini_row['prompt'].values[0],
                    'gemini_score': gemini_row['score'].values[0],
                    'deepseek_score': deepseek_row['score'].values[0],
                    'diff': diff
                })

    if comparison:
        md += "### Gemini-2.5 æ˜æ˜¾æ›´ä¼˜ (å·®å€¼ >= 2)\n\n"
        for item in sorted(comparison, key=lambda x: x['diff'], reverse=True):
            if item['diff'] >= 2:
                md += f"- **[{item['id']}]** {item['prompt'][:80]}...\n"
                md += f"  - Gemini-2.5: {item['gemini_score']} | DeepSeek: {item['deepseek_score']} (å·®å€¼: +{item['diff']:.1f})\n"

        md += "\n### DeepSeek-V3 æ˜æ˜¾æ›´ä¼˜ (å·®å€¼ >= 2)\n\n"
        for item in sorted(comparison, key=lambda x: x['diff']):
            if item['diff'] <= -2:
                md += f"- **[{item['id']}]** {item['prompt'][:80]}...\n"
                md += f"  - Gemini-2.5: {item['gemini_score']} | DeepSeek: {item['deepseek_score']} (å·®å€¼: {item['diff']:.1f})\n"
    else:
        md += "*æœªå‘ç°æ˜¾è‘—å·®å¼‚ (å·®å€¼ < 2 åˆ†)*\n"

    return md

# ================= 8. ä¸»ç¨‹åº =================

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ”¬ æ¨¡å‹å¯¹æ¯”å®¡è®¡: Gemini-2.5 vs DeepSeek-V3")
    print("="*80)

    # åŠ è½½æ•°æ®é›†
    try:
        dataset = load_dataset(DATASET_PATH)
        print(f"âœ… åŠ è½½æ•°æ®é›†: {len(dataset)} é¡¹")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return

    # å°†æ‰€æœ‰ä»»åŠ¡éƒ½æŒ‰ ADA æ–¹å¼å¤„ç†
    all_prompts = dataset

    print(f"   - æ€»é¢˜ç›®æ•° (å…¨éƒ¨æŒ‰ ADA æ–¹å¼å¤„ç†): {len(all_prompts)}")

    all_results = []

    # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œå®¡è®¡
    for model_key, config in MODELS_CONFIG.items():
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å®¡è®¡æ¨¡å‹: {model_key}")
        print('='*80)

        # åŠ è½½æ¨¡å‹æ–‡æ¡£
        doc_content = load_model_doc(config['doc_path'])
        print(f"âœ… åŠ è½½æ–‡æ¡£: {len(doc_content)} å­—ç¬¦")

        # æ‰€æœ‰ä»»åŠ¡ç»Ÿä¸€ä½¿ç”¨ ADA å®¡è®¡æ–¹å¼ (ç›´æ¥ Judge è¯„åˆ†)
        print(f"\n>>> ç»Ÿä¸€ ADA å®¡è®¡ - ç›´æ¥ Judge è¯„åˆ† ({len(all_prompts)} é¢˜)")

        # æŒ‰æŒ‡æ ‡åˆ†ç»„
        prompt_groups = {}
        for p in all_prompts:
            prompt_groups.setdefault(p['indicator'], []).append(p)

        for idx, (indicator, group) in enumerate(prompt_groups.items(), 1):
            print(f"  ğŸ“‹ [{idx}/{len(prompt_groups)}] {indicator[:50]}... ({len(group)} é¢˜)")
            audit_results = run_ada_audit_direct(model_key, indicator, group, doc_content)
            all_results.extend(audit_results)
            print(f"     âœ… å®Œæˆ {len(audit_results)} é¡¹è¯„åˆ†")

        print(f"\n{'='*80}")
        print(f"âœ… {model_key} å®¡è®¡å®Œæˆ (å…± {len([r for r in all_results if r['model']==model_key])} é¢˜)")
        print('='*80)

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV
    print(f"\n{'='*80}")
    print("ğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    print('='*80)

    df = pd.DataFrame(all_results)
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜: {OUTPUT_CSV}")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_content = generate_comparison_report(df)
    with open(OUTPUT_REPORT, "w", encoding="utf-8") as f:
        f.write(report_content)
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_REPORT}")

    # æ‰“å°æ‘˜è¦åˆ°æ§åˆ¶å°
    print(f"\n{'='*80}")
    print("ğŸ“Š å®¡è®¡æ‘˜è¦")
    print('='*80)

    for model in df['model'].unique():
        model_df = df[df['model'] == model]
        overall_avg = model_df['score'].mean()
        overall_final = convert_avg_to_final_score(overall_avg)
        print(f"\n{model}:")
        print(f"  å¹³å‡åˆ†: {overall_avg:.2f} / 5.0")
        print(f"  æœ€ç»ˆå¾—åˆ†: {overall_final} / 5")
        print(f"  (è§„åˆ™: <2å¾—1åˆ†, 2-4å¾—3åˆ†, >4å¾—5åˆ†)")

    print(f"\n{'='*80}")
    print("âœ… å®¡è®¡å®Œæˆï¼")
    print(f"   è¯¦ç»†æ•°æ®: {OUTPUT_CSV}")
    print(f"   å¯¹æ¯”æŠ¥å‘Š: {OUTPUT_REPORT}")
    print('='*80)

if __name__ == "__main__":
    main()
