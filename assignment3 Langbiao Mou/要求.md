# ADA End-to-End Pipeline Specification  
### (with Two-Stage Relevance Filtering + Claim Extraction + DeepSeek Scoring)

This document defines the complete software specification for implementing an  
Automated Document Analysis (ADA) pipeline for Assignment 3.  
Use this document directly in Condex / Cursor to auto-generate code.

---

# 1. Project Goal

Build an ADA pipeline that evaluates different AI models' documentation quality using  
the 5 required L4 indicators:

1. **L4_SafetyTradeoffs** – Safety objectives & value trade-offs articulated  
2. **L4_RedTeamingUpdates** – Red-teaming results inform alignment updates  
3. **L4_DomainFineTunes** – Domain-specific fine-tunes documented with safety constraints  
4. **L4_RollbackMechanisms** – Model update & rollback mechanisms exist  
5. **L4_PedagogyEvidence** – Pedagogy evidence documented  

The pipeline must:

- Load PDF/HTML documentation  
- Locate relevant sections (relevance filtering)  
- Extract claims + evidence spans using Claude  
- Score indicators using DeepSeek (0/1/2)  
- Save extracted JSON + scoring JSON  
- Aggregate scores for visualization  

---

# 2. System Architecture

Download docs → relevance filtering → ADA claim extraction → DeepSeek scoring
→ aggregate scores → generate plots → final report

---

# 3. File Structure

project_root/
docs/
data/
docs_meta.json
scripts/
ada_extract.py
relevance_filter.py
results/
extracted/
scored/
report/
analysis.md
figures/

---

# 4. docs_meta.json Format

```json
{
  "docs": [
    {
      "path": "docs/gpt4_system_card.pdf",
      "model": "OpenAI_GPT4",
      "title": "GPT-4 System Card",
      "url": "https://...",
      "indicators": ["L4_SafetyTradeoffs", "L4_RedTeamingUpdates"]
    }
  ]
}
5. Two-Stage Relevance Filtering

(Major improvement to handle long documents)

Large system cards (20–80+ pages) cannot be fed directly into ADA.
We must extract only the most relevant sections.
5.1 Stage 1 — Keyword-Based Page Scoring

Define keyword lists per indicator:
KEYWORDS = {
  "L4_SafetyTradeoffs": ["safety", "risk", "trade-off", "balance", "utility", "helpfulness", "value"],
  "L4_RedTeamingUpdates": ["red-teaming", "adversarial", "jailbreak", "alignment update", "mitigation"],
  "L4_DomainFineTunes": ["fine-tune", "domain-specific", "vertical", "specialized", "safety constraints"],
  "L4_RollbackMechanisms": ["rollback", "revert", "version history", "incident", "suspension"],
  "L4_PedagogyEvidence": ["pedagogy", "instructional", "curriculum", "learning science", "education"]
}
Algorithm steps:

Split PDF into pages

Count keyword matches (case-insensitive)

Produce a score for each page

Select top K pages (default K = 5)

Combine them into combined_text

This reduces document length dramatically while keeping relevant evidence.

5.2 Stage 2 — LLM Refinement (optional)

Use Claude / GPT to further refine the selected text:

Prompt:

Keep only sentences strongly relevant to indicator {indicator_id}.
Remove irrelevant content. Output refined text only.

This creates the final_clean_text for ADA extraction.

ADA Claim Extraction (Claude)
Required Prompt Template
You are performing Automated Document Analysis (ADA) for AI ethics.

Indicator: {indicator_id}
Document title: {title}
Model: {model}

Task:
1. Extract ALL claims relevant to this indicator.
2. For each claim return:
   - summary (your own words)
   - evidence_span (verbatim quote from the document)
   - page_or_section
   - confidence (0–1)
   - subtags (optional list)

Return ONLY a JSON object in this structure:

{
  "indicator_id": "...",
  "model_name": "...",
  "doc_title": "...",
  "claims": [
    {
      "summary": "...",
      "evidence_span": "...",
      "page_or_section": "...",
      "confidence": 0.92,
      "subtags": []
    }
  ]
}
The pipeline must call Claude (API) with this prompt and the filtered text.
7. DeepSeek Scoring (0/1/2)

DeepSeek is used to judge the extracted claims.

Scoring Rubric

Score 2 – Strong Evidence

Clear, explicit support

Multiple relevant spans

High confidence claims

Concrete mechanisms described

Score 1 – Partial Evidence

Some relevant content

Vague or incomplete

Only one weak span

Score 0 – No Evidence

No meaningful support

Only generic statements

No verbatim evidence

Required JSON Output
{
  "indicator_id": "...",
  "model_name": "...",
  "score": 1,
  "rationale": "..."
}

8. Aggregation Requirements

Pipeline must generate a final table:

| model | indicator | score | rationale |

Also export CSV, and optionally generate bar charts or radar charts.
9. Functions Condex Must Implement

The generated code must implement:
load_pdf_text(path)
page_relevance_score(text, keywords)
extract_relevant_pages(pdf_path, indicator_id, top_k)
refine_relevant_text(raw_text, indicator_id)   # optional
ada_extract_for_doc(indicator_id, combined_text, meta)
save_extraction(result)
load_docs_registry(path="data/docs_meta.json")
run_ada_pipeline()

10. Expected Output from Condex

Condex should generate:

relevance_filter.py

ada_extract.py

utils.py

main pipeline script

logging + error handling

The pipeline must work end-to-end with downloaded PDFs.

11. Deliverables for the Assignment

ADA claim extraction JSON (Claude)

Scoring JSON (DeepSeek)

Aggregated CSV

Comparison bar chart

analysis.md writeup

Appendix with evidence spans